# ğŸš€ Transformers from Scratch
### *Implementing "Attention Is All You Need" in PyTorch & TensorFlow*

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/Framework-PyTorch-ee4c2c.svg)](https://pytorch.org/)
[![TensorFlow](https://img.shields.io/badge/Framework-TensorFlow-FF6F00.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

Welcome to my dedicated repository for mastering **Transformer Architectures**. This project documents the complete journey of implementing the seminal "Attention Is All You Need" paper from the ground up. By building every component in both **PyTorch** and **TensorFlow**, I explore the mathematical foundations and framework-specific nuances that power modern Large Language Models (LLMs).

---

## ğŸ“Œ Project Overview
The Transformer architecture revolutionized NLP and Computer Vision. This repository goes beyond simple architecture to cover the entire model lifecycle:
* **Core Mechanics**: Scaled Dot-Product Attention & Multi-Head Projections.
* **Architecture Variants**: Encoder-only (BERT), Decoder-only (GPT), and Full Stack (Vanilla).
* **Data Pipeline**: Custom Tokenizers (BPE & WordPiece) and Special Token handling.
* **Optimization**: Learning Rate Warmup, Label Smoothing, and Cross-Entropy variants.

---

## ğŸ› ï¸ Repository Structure

```text
Transformers-from-Scratch/
â”œâ”€â”€ 01_Core_Components/          # Atomic Units of Attention
â”‚   â”œâ”€â”€ PyTorch/                 # MHA & Scaled Dot-Product Implementations
â”‚   â”œâ”€â”€ TensorFlow/              # Keras-based Core Layers
â”‚   â””â”€â”€ Positional_Encoding.py   # Sinusoidal Order Injection logic
â”œâ”€â”€ 02_Architectures/            # Model Assemblies
â”‚   â”œâ”€â”€ Vanilla_Transformer/     # The 2017 Original (Encoder-Decoder)
â”‚   â”œâ”€â”€ BERT/                    # Encoder-only (Bidirectional Context)
â”‚   â””â”€â”€ GPT/                     # Decoder-only (Autoregressive Generation)
â”œâ”€â”€ 03_Training_Pipeline/        # The "Engine Room"
â”‚   â”œâ”€â”€ Tokenization/            # BPE & WordPiece implemented from scratch
â”‚   â”œâ”€â”€ Optimization/            # Warmup Schedulers & Label Smoothing
â”‚   â””â”€â”€ Evaluation/              # BLEU Score & Perplexity Metrics
â”œâ”€â”€ 04_Projects/                 # End-to-End Applications
â”‚   â”œâ”€â”€ Machine_Translation/     # Full Seq2Seq NMT (English-to-Hindi)
â”‚   â”œâ”€â”€ Sentiment_Analysis/      # BERT-based Text Classification
â”‚   â””â”€â”€ GPT_Story_Gen/           # Mini-GPT for Autoregressive Generation
â”œâ”€â”€ 05_Notebooks/                # Visualization & Analysis
â”‚   â””â”€â”€ Visualizing_Attention.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
