# ğŸš€ Transformers from Scratch
### *Implementing "Attention Is All You Need" in PyTorch & TensorFlow*

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/Framework-PyTorch-ee4c2c.svg)](https://pytorch.org/)
[![TensorFlow](https://img.shields.io/badge/Framework-TensorFlow-FF6F00.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

Welcome to my dedicated repository for mastering **Transformer Architectures**. This project documents my journey of implementing the seminal "Attention Is All You Need" paper from the ground up, using both **PyTorch** and **TensorFlow** to understand the cross-framework nuances of Attention Mechanisms.

---

## ğŸ“Œ Project Overview
The Transformer architecture revolutionized NLP and Computer Vision. In this repository, I break down:
* **Self-Attention Mechanisms** (The "DNA" of modern AI)
* **Multi-Head Projections** for parallel representation
* **Framework Comparisons**: Implementing core logic in both `torch.nn` and `tf.keras`.

---

## ğŸ› ï¸ Repository Structure

```text
Transformers-from-Scratch/
â”œâ”€â”€ 01_Core_Components/
â”‚   â”œâ”€â”€ PyTorch/
â”‚   â”‚   â”œâ”€â”€ Scaled_Dot_Product_Attention.py
â”‚   â”‚   â””â”€â”€ Multi_Head_Attention.py
â”‚   â”œâ”€â”€ TensorFlow/
â”‚   â”‚   â”œâ”€â”€ Scaled_Dot_Product_Attention.py
â”‚   â”‚   â””â”€â”€ Multi_Head_Attention.py
â”‚   â””â”€â”€ Positional_Encoding.py
â”œâ”€â”€ 02_Architectures/
â”‚   â”œâ”€â”€ Vanilla_Transformer/              # The Original Paper (2017)
â”‚   â”œâ”€â”€ BERT/                             # Encoder-only (Masked LM)
â”‚   â””â”€â”€ GPT/                              # Decoder-only (Generative)
â”œâ”€â”€ 03_Projects/
â”‚   â”œâ”€â”€ Machine_Translation/              # Seq2Seq translation
â”‚   â””â”€â”€ Sentiment_Classifier/             # BERT fine-tuning
â”œâ”€â”€ 04_Notebooks/
â”‚   â””â”€â”€ Visualizing_Attention_Heads.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
