# ğŸš€ Transformers from Scratch
### *Implementing "Attention Is All You Need" from the ground up*

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/Framework-PyTorch-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

Welcome to my dedicated repository for mastering **Transformer Architectures** and **Attention Mechanisms**. This project documents my journey of implementing the seminal "Attention Is All You Need" paper, moving from mathematical foundations to Large Language Models (LLMs).

---

## ğŸ“Œ Project Overview
The Transformer architecture revolutionized NLP and Computer Vision by replacing recurrence with **Self-Attention**. In this repository, I break down the complexity of:
* **Self-Attention Mechanisms** (The "DNA" of modern AI)
* **Multi-Head Projections** for parallel representation
* **Encoder-Decoder Stacks** for Seq2Seq tasks

---

## ğŸ› ï¸ Repository Structure

```text
Transformers-from-Scratch/
â”œâ”€â”€ 01_Core_Components/
â”‚   â”œâ”€â”€ Scaled_Dot_Product_Attention.py   <-- ğŸ Start Here!
â”‚   â”œâ”€â”€ Multi_Head_Attention.py
â”‚   â””â”€â”€ Positional_Encoding.py
â”œâ”€â”€ 02_Architectures/
â”‚   â”œâ”€â”€ Vanilla_Transformer/              # The Original Paper (2017)
â”‚   â”œâ”€â”€ BERT/                             # Encoder-only (Masked LM)
â”‚   â””â”€â”€ GPT/                              # Decoder-only (Generative)
â”œâ”€â”€ 03_Projects/
â”‚   â”œâ”€â”€ Machine_Translation/              # Seq2Seq translation
â”‚   â””â”€â”€ Sentiment_Classifier/             # BERT fine-tuning
â”œâ”€â”€ 04_Notebooks/
â”‚   â””â”€â”€ Visualizing_Attention_Heads.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
